{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import os\n",
    "color = sns.color_palette()\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = 999\n",
    "\n",
    "from itertools import chain\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import Imputer, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadInData(object):\n",
    "    def __init__(self, path_to_data, properties_file, train_file):\n",
    "        # Input path\n",
    "        self.path_to_data = path_to_data\n",
    "        \n",
    "        # Data type definition\n",
    "        self.dtype_dict = {\n",
    "            'parcelid':                        np.int32,\n",
    "            'airconditioningtypeid':           np.float64,\n",
    "            'architecturalstyletypeid':        np.float64,\n",
    "            'basementsqft':                    np.float64,\n",
    "            'bathroomcnt':                     np.float64,\n",
    "            'bedroomcnt':                      np.float64,\n",
    "            'buildingclasstypeid':             np.float64,\n",
    "            'buildingqualitytypeid':           np.float64,\n",
    "            'calculatedbathnbr':               np.float64,\n",
    "            'decktypeid':                      np.float64,\n",
    "            'finishedfloor1squarefeet':        np.float64,\n",
    "            'calculatedfinishedsquarefeet':    np.float64,\n",
    "            'finishedsquarefeet12':            np.float64,\n",
    "            'finishedsquarefeet13':            np.float64,\n",
    "            'finishedsquarefeet15':            np.float64,\n",
    "            'finishedsquarefeet50':            np.float64,\n",
    "            'finishedsquarefeet6':             np.float64,\n",
    "            'fips':                            np.float64,\n",
    "            'fireplacecnt':                    np.float64,\n",
    "            'fullbathcnt':                     np.float64,\n",
    "            'garagecarcnt':                    np.float64,\n",
    "            'garagetotalsqft':                 np.float64,\n",
    "            'hashottuborspa':                  np.bool_,\n",
    "            'heatingorsystemtypeid':           np.float64,\n",
    "            'latitude':                        np.float64,\n",
    "            'longitude':                       np.float64,\n",
    "            'lotsizesquarefeet':               np.float64,\n",
    "            'poolcnt':                         np.float64,\n",
    "            'poolsizesum':                     np.float64,\n",
    "            'pooltypeid10':                    np.float64,\n",
    "            'pooltypeid2':                     np.float64,\n",
    "            'pooltypeid7':                     np.float64,\n",
    "            'propertycountylandusecode':       str,\n",
    "            'propertylandusetypeid':           np.float64,\n",
    "            'propertyzoningdesc':              str,\n",
    "            'rawcensustractandblock':          np.float64,\n",
    "            'regionidcity':                    np.float64,\n",
    "            'regionidcounty':                  np.float64,\n",
    "            'regionidneighborhood':            np.float64,\n",
    "            'regionidzip':                     np.float64,\n",
    "            'roomcnt':                         np.float64,\n",
    "            'storytypeid':                     np.float64,\n",
    "            'threequarterbathnbr':             np.float64,\n",
    "            'typeconstructiontypeid':          np.float64,\n",
    "            'unitcnt':                         np.float64,\n",
    "            'yardbuildingsqft17':              np.float64,\n",
    "            'yardbuildingsqft26':              np.float64,\n",
    "            'yearbuilt':                       np.float64,\n",
    "            'numberofstories':                 np.float64,\n",
    "            'fireplaceflag':                   np.bool_,\n",
    "            'structuretaxvaluedollarcnt':      np.float64,\n",
    "            'taxvaluedollarcnt':               np.float64,\n",
    "            'assessmentyear':                  np.float64,\n",
    "            'landtaxvaluedollarcnt':           np.float64,\n",
    "            'taxamount':                       np.float64,\n",
    "            'taxdelinquencyflag':              str,\n",
    "            'taxdelinquencyyear':              np.float64,\n",
    "            'censustractandblock':             np.float64,\n",
    "        }\n",
    "        \n",
    "        # Categorical columns with many unique values\n",
    "        self.mean_cols = ['propertyzoningdesc', \n",
    "             'propertycountylandusecode', \n",
    "             'rawcensustractandblock', \n",
    "             'regionidcity',\n",
    "             'regionidneighborhood',\n",
    "             'regionidzip',\n",
    "             'censustractandblock',\n",
    "             'assessmentyear'\n",
    "            ]\n",
    "        \n",
    "        for column in self.mean_cols:\n",
    "            self.dtype_dict[column] = 'category'\n",
    "        \n",
    "        # File names\n",
    "        self.properties_file = properties_file\n",
    "        self.train_file = train_file\n",
    "        \n",
    "        # Imported datasets\n",
    "        self.properties = pd.DataFrame()\n",
    "        self.train_driver = pd.DataFrame()\n",
    "        \n",
    "    def import_data(self):\n",
    "        self.train_driver = pd.read_csv(os.path.join(self.path_to_data, self.train_file), \n",
    "                                        parse_dates = [\"transactiondate\"])\n",
    "        \n",
    "        self.properties = pd.read_csv(os.path.join(self.path_to_data, self.properties_file),\n",
    "                                      dtype=self.dtype_dict)\n",
    "        self.properties = self.properties[self.properties.latitude.notnull()]\n",
    "        #Need to convert categorical variables to categorical data types    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CreateDatasets(ReadInData):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CreateDatasets, self).__init__(**kwargs)\n",
    "        \n",
    "        # Compiled datasets\n",
    "        self.y_train = pd.DataFrame()\n",
    "        self.scoring_driver = pd.DataFrame()\n",
    "        self.Xs = {'x_train': pd.DataFrame(), 'x_scoring': pd.DataFrame()}\n",
    "\n",
    "        # Special columns to treat differently\n",
    "        self.drop_indices = ['parcelid']\n",
    "        self.y_col = 'logerror'\n",
    "\n",
    "        self.mean_col_dfs_ = {}\n",
    "    \n",
    "    def calculate_categorical_means(self, df, cat_column):\n",
    "        # Calculate mean logerrors for categorical variables\n",
    "        categorical_means = (df[['logerror', cat_column]]\n",
    "                                          .groupby(cat_column)\n",
    "                                          .agg({'logerror': 'mean'})\n",
    "                                          .rename(columns={'logerror': cat_column+'_mean'}))\n",
    "        categorical_means.reset_index(inplace=True) \n",
    "        categorical_means[cat_column+'_mean'].fillna(0, inplace=True)\n",
    "        return categorical_means\n",
    "    \n",
    "    def create_scoring_driver(self, properties_df):\n",
    "        # Start by identifying unique parcels\n",
    "        parcels = properties_df.parcelid.unique()\n",
    "        \n",
    "        # Adding to a dataframe with a dummy variable that will allow a cross join in the next step\n",
    "        parcels_df = pd.DataFrame({'parcelid': parcels, 'dummy': np.zeros(len(parcels))})\n",
    "\n",
    "        #Create a row for each parcel for each month to be scored\n",
    "        trxn_months = pd.DataFrame({'transactiondate': ['2016/10/01', '2016/11/01', '2016/12/01', '2017/10/01', '2017/11/01', '2017/12/01'],\n",
    "                            'logerror': np.zeros(6),\n",
    "                            'dummy': np.zeros(6)})\n",
    "\n",
    "        trxn_months.transactiondate = pd.to_datetime(trxn_months.transactiondate)\n",
    "\n",
    "        # Finally create the driver for scoring\n",
    "        return pd.merge(parcels_df, trxn_months, how='outer', on='dummy').drop('dummy', axis=1)\n",
    "    \n",
    "    def run(self):\n",
    "        self.import_data()\n",
    "            \n",
    "        train = pd.merge(self.properties, self.train_driver, how='left', on='parcelid')\n",
    "        self.y_train = train[self.y_col]\n",
    "        self.Xs['x_train'] = train.drop(self.drop_indices+[self.y_col], axis=1)\n",
    "        \n",
    "        self.scoring_driver = self.create_scoring_driver(self.properties)\n",
    "        self.scoring = pd.merge(self.scoring_driver, self.properties, how='left', on='parcelid')\n",
    "        self.Xs['x_scoring'] = self.scoring.drop(self.drop_indices, axis=1)\n",
    "        \n",
    "        # Calculate categorical means\n",
    "        for column in self.mean_cols:\n",
    "            self.mean_col_dfs_[column] = self.calculate_categorical_means(df=train, cat_column=column)\n",
    "        \n",
    "        self.Xs['x_train'] = self.Xs['x_train'][self.Xs['x_train']['transactiondate'].notnull()]\n",
    "        self.y_train = self.y_train[self.y_train.notnull()]\n",
    "        \n",
    "        # Perform transforms that won't work in the pipeline:\n",
    "        # - Drop transactiondate\n",
    "        # - Append categorical means to training and scoring x files\n",
    "        for key, dataset in self.Xs.items():\n",
    "            # Transaction month\n",
    "            self.Xs[key]['transaction_month'] = self.Xs[key]['transactiondate'].dt.month\n",
    "            self.Xs[key].drop('transactiondate', axis=1, inplace=True)\n",
    "            for column in self.mean_cols:\n",
    "                self.Xs[key] = pd.merge(self.Xs[key], self.mean_col_dfs_[column], how='left', on=column)\n",
    "                self.Xs[key].drop(column, axis=1, inplace=True)\n",
    "                self.Xs[key][column+'_mean_miss'] = self.Xs[key][column+'_mean'].isnull()\n",
    "                self.Xs[key][column+'_mean'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drivers = CreateDatasets(path_to_data='/Users/socrteas/Documents/Kaggle/zillow/data/', properties_file='properties_2016.csv', train_file='train_2016_v2.csv')\n",
    "drivers.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseFeatureTransformer(BaseEstimator, TransformerMixin):\n",
    "#     def __init__(self, transforms = [(\"impute\", Imputer()), (\"scale\", StandardScaler())]):\n",
    "    def __init__(self):\n",
    "#         self.transforms = transforms\n",
    "        \n",
    "        self.nan_subs = {'airconditioningtypeid': 0, #getdummies\n",
    "            'architecturalstyletypeid': 0, #getdummies\n",
    "            'basementsqft': 0,\n",
    "            'bathroomcnt': 0, \n",
    "            'bedroomcnt': 0, \n",
    "            'buildingclasstypeid': 0, #get_dummies\n",
    "            'buildingqualitytypeid': 0,  #get_dummies\n",
    "            'calculatedbathnbr': 0, \n",
    "            'decktypeid': 0, #actually binary\n",
    "            'finishedfloor1squarefeet': 1348, \n",
    "            'calculatedfinishedsquarefeet': 1773,\n",
    "            'finishedsquarefeet12': 1745, \n",
    "            'finishedsquarefeet13': 1404, \n",
    "            'finishedsquarefeet15': 2380,\n",
    "            'finishedsquarefeet50': 1355, \n",
    "            'finishedsquarefeet6': 2303, \n",
    "            'fips': 0, #getdummies\n",
    "            'fireplacecnt': 0,\n",
    "            'fullbathcnt': 0, \n",
    "            'garagecarcnt': 0, \n",
    "            'garagetotalsqft': 0, \n",
    "            'hashottuborspa': False,\n",
    "            'heatingorsystemtypeid': 0, #getdummies\n",
    "            'latitude': 0, \n",
    "            'longitude': 0, \n",
    "            'lat_bins': 0,\n",
    "            'lon_bins': 0,\n",
    "            'lotsizesquarefeet': 0, #Lots of heteroscedasticity here, high values have low logerrors\n",
    "            'poolcnt': 0, \n",
    "            'poolsizesum': 0, #Could use 520 (mean) here for rows where poolcnt = 1\n",
    "            'pooltypeid10': 0, \n",
    "            'pooltypeid2': 0, \n",
    "            'pooltypeid7': 0,\n",
    "#             'propertycountylandusecode': '0', \n",
    "            'propertylandusetypeid': 0,\n",
    "#             'propertyzoningdesc': '0', \n",
    "#             'rawcensustractandblock': 0, \n",
    "#             'regionidcity': 0,\n",
    "            'regionidcounty': 0, #getdummies\n",
    "#             'regionidneighborhood': 0, \n",
    "#             'regionidzip': 0, \n",
    "            'roomcnt': 0,\n",
    "            'storytypeid': 0, \n",
    "            'threequarterbathnbr': 0, \n",
    "            'typeconstructiontypeid': 0, #getdummies\n",
    "            'unitcnt': 1, \n",
    "            'yardbuildingsqft17': 0, \n",
    "            'yardbuildingsqft26': 0, \n",
    "            'yearbuilt': 1968,\n",
    "            'numberofstories': 1, \n",
    "            'fireplaceflag': False, \n",
    "            'structuretaxvaluedollarcnt': 180000,  #Lots of heteroscedasticity here, high values have low logerrors\n",
    "            'taxvaluedollarcnt': 450000,   #Lots of heteroscedasticity here, high values have low logerrors\n",
    "#             'assessmentyear': 2015,\n",
    "            'landtaxvaluedollarcnt': 280000, #Lots of heteroscedasticity here, high values have low logerrors\n",
    "            'taxamount': 6000,   #Lots of heteroscedasticity here, high values have low logerrors\n",
    "            'taxdelinquencyflag': 'N', \n",
    "            'taxdelinquencyyear': 16,\n",
    "#             'censustractandblock': '0'\n",
    "           }\n",
    "        \n",
    "        self.taxdelinquencyflag_mapping = {'Y': 1, 'N': 0}\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # Create latitude and longitude bins\n",
    "        trash1, self.lat_bins_ = pd.cut(X.latitude, bins=100, labels=False, retbins=True)\n",
    "        trash2, self.lon_bins_ = pd.cut(X.longitude, bins=100, labels=False, retbins=True)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "#         # Transaction month\n",
    "#         X['transaction_month'] = X['transactiondate'].dt.month\n",
    "#         X.drop('transactiondate', axis=1, inplace=True)\n",
    "        \n",
    "        # Bin latitude and longitude to avoid overfit\n",
    "        # Need to create lat_bins and lon_bins as class attributes when the class is created\n",
    "        X['lat_bins'] = pd.cut(X.latitude, self.lat_bins_, labels=np.arange(100))\n",
    "        X['lon_bins'] = pd.cut(X.longitude, self.lon_bins_, labels=np.arange(100))\n",
    "        \n",
    "        # Create missing indicators\n",
    "        for key in self.nan_subs:\n",
    "            X[key+'_miss'] = X[key].isnull()\n",
    "            \n",
    "        # Fill NaN's with imputations\n",
    "        X.fillna(self.nan_subs, inplace=True)\n",
    "        \n",
    "        # Convert ordinal characters to ordinal numeric\n",
    "        X.taxdelinquencyflag = X.taxdelinquencyflag.map(self.taxdelinquencyflag_mapping)\n",
    "        \n",
    "#         self.columns_ = X.columns\n",
    "#         self.feature_columns_ = X.columns.drop(self.drop_cols)\n",
    "#         self.pipe = Pipeline(self.transforms).fit(X.ix[:, self.feature_columns_])\n",
    "        \n",
    "#         cat_cols = X.drop(self.non_cat_columns_.values, 1).reset_index(drop = True)\n",
    "#         scaled_df = pd.concat([scaled_cols, cat_cols], axis = 1)\n",
    "#         final_matrix = (pd.get_dummies(scaled_df)\n",
    "#                         .reindex(columns = self.transformed_columns_)\n",
    "#                         .fillna(0).as_matrix())\n",
    "        return X.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature transformation and training pipeline\n",
    "preprocess = BaseFeatureTransformer()\n",
    "params = {'subsample': 0.5, 'min_samples_leaf': 100, 'learning_rate': 0.02, 'max_features': 0.3, 'max_depth': 8}\n",
    "gbm = GradientBoostingRegressor(loss='lad', random_state=90210)\n",
    "pipe = Pipeline([\n",
    "    (\"preprocess\", preprocess), \n",
    "    (\"gbm\", gbm)\n",
    "])\n",
    "\n",
    "#                       dict(gbm__n_estimators = [500, 800, 1000, 1200],\n",
    "#                            gbm__learning_rate = [0.02, 0.01],\n",
    "#                            gbm__max_depth = [4, 6, 8],\n",
    "#                            gbm__min_samples_leaf = [50, 100],\n",
    "#                            gbm__subsample = [0.5],\n",
    "#                            gbm__max_features = [0.5, 0.3, 0.1])\n",
    "\n",
    "param_grid = {'n_estimators': [50, 80],\n",
    "              'learning_rate': [0.02, 0.01],\n",
    "              'max_depth': [4, 6, 8],\n",
    "              'min_samples_leaf': [50, 100],\n",
    "              'subsample': [0.5],\n",
    "              'max_features': [0.5, 0.3]\n",
    "              }\n",
    "\n",
    "# fit model\n",
    "gbm_cv = GridSearchCV(gbm,\n",
    "                      param_grid,\n",
    "#                       cv = 5, # Default is 3-fold CV\n",
    "                      n_jobs = 4,\n",
    "                      scoring = make_scorer(mean_absolute_error))\n",
    "\n",
    "test = BaseFeatureTransformer().fit_transform(drivers.Xs['x_train'])\n",
    "gbm_cv.fit(test, drivers.y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(drivers.Xs['x_train'].columns)\n",
    "print(drivers.Xs['x_train'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gbm_cv.predict(drivers.Xs['x_scoring'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Modeling ideas\n",
    "Get dummies for aforementioned columns with several unique values\n",
    "\n",
    "Remove outlier logerrors\n",
    "\n",
    "Test difference on leaderboard when holdout validation isn't good. Does leaderboard position actually suffer?\n",
    "\n",
    "Include multiplier interactions (create class in pipeline to create)\n",
    "\n",
    "Stack models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
